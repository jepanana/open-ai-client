use std::collections::HashMap;

use serde::{Deserialize, Serialize};

use crate::common::{ChatModel, MessageRole};

use super::{ChatResponseFormat, Tool, ToolChoice};

/// Request to the Chat API
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
#[serde(default)]
pub struct CreateChatCompletionRequest {
    /// ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)
    /// table for details on which models work with the Chat API.
    pub model: String,

    /// A list of messages comprising the conversation so far.
    /// [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
    pub messages: Vec<ChatRequestMessage>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim.
    ///
    ///[See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,

    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer)
    /// to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
    /// generated by the model prior to sampling. The exact effect will vary per model,
    /// but values between -1 and 1 should decrease or increase likelihood of selection;
    /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    #[serde(skip_serializing_if = "HashMap::is_empty")]
    pub logit_bias: HashMap<String, i32>,

    /// The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the chat completion.
    /// The total length of input tokens and generated tokens is limited by the model's context length.
    /// [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
    /// for counting tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<i32>,

    /// How many chat completion choices to generate for each input message.
    /// Note that you will be charged based on the number of generated tokens across all of the choices.
    /// Keep n as 1 to minimize costs.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<i64>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,
    /// increasing the model's likelihood to talk about new topics.
    ///
    /// [See more information about frequency and presence penalties](https://platform.openai.com/docs/guides/text-generation/parameter-details)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,

    /// The format the model should return
    ///
    /// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
    ///
    /// <b>Important</b>: when using JSON mode, you <b>must</b> also instruct the model to produce JSON yourself via a system or user message.
    /// Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit,
    /// resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off
    /// if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<ChatResponseFormat>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically,
    /// such that repeated requests with the same `seed` and parameters should return the same result.
    /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub seed: Option<i64>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Vec::is_empty")]
    pub stop: Vec<String>,

    /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
    /// (server-sent events)[https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format]
    /// as they become available, with the stream terminated by a data: [DONE] message
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// What sampling temperature to use, between 0 and 2.
    /// Higher values like 0.8 will make the output more random,
    /// while lower values like 0.2 will make it more focused and deterministic.
    ///
    /// We generally recommend altering this or top_p but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    ///An alternative to sampling with temperature, called nucleus sampling,
    /// where the model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    ///
    /// We generally recommend altering this or temperature but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// A list of tools the model may call. Currently, only functions are supported as a tool.
    /// Use this to provide a list of functions the model may generate JSON inputs for.
    #[serde(skip_serializing_if = "Vec::is_empty")]
    pub tools: Vec<Tool>,

    /// Controls which (if any) function is called by the model.
    /// `none` means the model will not call a function and instead generates a message.
    /// `auto` means the model can pick between generating a message or calling a function.
    /// Specifying a particular function via `{"type: "function", "function": {"name": "my_function"}}`
    /// forces the model to call that function.
    ///
    /// `none` is the default when no functions are present. `auto` is the default if functions are present.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<ToolChoice>,

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse
    /// [More](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}

impl CreateChatCompletionRequest {
    /// Creates a new [`ChatCompletionRequest`] with a single message from a given user query.
    pub fn from_user_query<S>(query: S) -> Self
    where
        S: Into<String>,
    {
        let messages = vec![ChatRequestMessage {
            content: query.into(),
            ..Default::default()
        }];

        Self {
            messages,
            ..Default::default()
        }
    }

    /// Creates a new [`ChatCompletionRequest`] from an multiple messages.
    pub fn from_messages(messages: &[ChatRequestMessage]) -> Self {
        Self {
            messages: messages.to_vec(),
            ..Default::default()
        }
    }

    /// Sets the model to use for the request.
    pub fn set_model(mut self, model: ChatModel) -> Self {
        self.model = model.to_string();
        self
    }

    /// Sets the messages for the request.
    pub fn set_messages(mut self, messages: Vec<ChatRequestMessage>) -> Self {
        self.messages = messages;
        self
    }

    /// Adds a message to the request.
    pub fn add_message(mut self, message: ChatRequestMessage) -> Self {
        self.messages.push(message);
        self
    }

    /// Sets max tokens for chat gpt model.
    pub fn set_max_tokens(mut self, max_tokens: i32) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }
}

/// Message comprising the conversation
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct ChatRequestMessage {
    /// The role of the messages author. One of `system`, `user`, `assistant`, or `function`
    pub role: MessageRole,

    /// The contents of the message. `content` is required for all messages,
    /// and may be null for assistant messages with function calls.
    pub content: String,

    /// The name of the author of this message. `name` is required if role is `function`,
    /// and it should be the name of the function whose response is in the `content`.
    /// May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(default)]
    pub name: Option<String>,

    /// The name and arguments of a function that should be called, as generated by the model.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(default)]
    pub function_call: Option<String>,
}

impl ChatRequestMessage {
    /// Creates a new [`ChatRequestMessage`] with the given content and role.
    pub fn new(role: MessageRole, content: &str) -> Self {
        Self {
            role,
            content: content.to_string(),
            ..Default::default()
        }
    }

    /// Creates a new [`ChatRequestMessage`] with the given content and system role.
    pub fn system_message<S>(content: S) -> Self
    where
        S: Into<String>,
    {
        Self {
            role: MessageRole::System,
            content: content.into(),
            ..Default::default()
        }
    }

    /// Creates a new [`ChatRequestMessage`] with the given content and user role.
    pub fn user_message<S>(content: S) -> Self
    where
        S: Into<String>,
    {
        Self {
            role: MessageRole::User,
            content: content.into(),
            ..Default::default()
        }
    }

    /// Creates a new [`ChatRequestMessage`] with the given content and assistant role.
    pub fn assistant_message<S>(content: S) -> Self
    where
        S: Into<String>,
    {
        Self {
            role: MessageRole::Assistant,
            content: content.into(),
            ..Default::default()
        }
    }

    /// Sets the function call for the message.
    pub fn set_function_call<S>(mut self, name: S, function_call: String) -> Self
    where
        S: Into<String>,
    {
        self.name = Some(name.into());
        self.function_call = Some(function_call);
        self
    }
}

#[cfg(test)]
mod tests {
    use crate::{FunctionChoice, ToolChoiceObject};

    use super::*;
    use serde_json::json;

    #[test]
    fn serializes_request_correctly() {
        let request = CreateChatCompletionRequest {
            model: ChatModel::GPT3_5Turbo.to_string(),
            messages: vec![
                ChatRequestMessage {
                    role: MessageRole::System,
                    content: "You are a helpful assistant.".to_string(),
                    ..Default::default()
                },
                ChatRequestMessage {
                    role: MessageRole::User,
                    content: "Hello!".to_string(),
                    ..Default::default()
                },
            ],
            tools: vec![],
            tool_choice: Some(ToolChoice::Object(ToolChoiceObject {
                _type: Some("function".to_string()),
                function: Some(FunctionChoice {
                    name: "my_function".to_string(),
                }),
            })),
            ..Default::default()
        };

        let request_json = serde_json::to_string(&request).unwrap();

        let json = json!({
          "model": "gpt-3.5-turbo",
          "messages": [
            {
              "role": "system",
              "content": "You are a helpful assistant."
            },
            {
              "role": "user",
              "content": "Hello!"
            }
          ],
          "tools": [],
          "tool_choice": {
            "type": "function",
            "function": {
              "name": "my_function"
            }
          }
        });

        assert_eq!(request_json, json.to_string());
    }
}
